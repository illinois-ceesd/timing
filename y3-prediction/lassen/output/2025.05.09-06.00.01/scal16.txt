Customizing shell for Lassen.

The following have been reloaded with a version change:
  1) base-gcc/8.3.1 => base-gcc/11.2.1


Lmod is automatically replacing "clang/ibm-16.0.6-cuda-11.2.0-gcc-8.3.1" with
"gcc/10.2.1".


Due to MODULEPATH changes, the following have been reloaded:
  1) spectrum-mpi/rolling-release

Activating 'timing.pilot' environment for '/p/gpfs1/mtcampbe/CEESD/AutomatedTesting/RepoMonitoring/pilot-timing/emirge/miniforge3/bin/conda'.
/p/gpfs1/mtcampbe/CEESD/AutomatedTesting/RepoMonitoring/pilot-timing/y3-prediction-scaling-run/scalability_test
/p/gpfs1/mtcampbe/CEESD/AutomatedTesting/RepoMonitoring/pilot-timing/y3-prediction-scaling-run/scalability_test
Driver directory: /p/gpfs1/mtcampbe/CEESD/AutomatedTesting/RepoMonitoring/pilot-timing/y3-prediction-scaling-run
Thu May  8 12:34:05 PDT 2025
Running parallel timing tests...
* Running prediction-scalability test in scalability_test.
** Running prediction-scalability_np16 on 16 ranks.
++ runoptions='-n 16'
++ [[ ! -z '' ]]
++ jsrun -g 1 -a 1 -n 16 bash /p/gpfs1/mtcampbe/CEESD/AutomatedTesting/RepoMonitoring/pilot-timing/y3-prediction-scaling-run/emirge/mirgecom/scripts/lassen-parallel-spawner.sh python -u -O -m mpi4py driver.py -c prediction-scalability_np16 -g /p/gpfs1/mtcampbe/CEESD/AutomatedTesting/RepoMonitoring/pilot-timing/y3-prediction-scaling-run/scalability_test/log_data -i run_params_np16.yaml --log --lazy
Custom casename 'prediction-scalability_np16'
Custom casename 'prediction-scalability_np16'
Custom casename 'prediction-scalability_np16'
Custom casename 'prediction-scalability_np16'
Custom casename 'prediction-scalability_np16'
Custom casename 'prediction-scalability_np16'
Custom casename 'prediction-scalability_np16'
Custom casename 'prediction-scalability_np16'
Custom casename 'prediction-scalability_np16'
Custom casename 'prediction-scalability_np16'
Custom casename 'prediction-scalability_np16'
Custom casename 'prediction-scalability_np16'
Custom casename 'prediction-scalability_np16'
Custom casename 'prediction-scalability_np16'
Custom casename 'prediction-scalability_np16'
Custom casename 'prediction-scalability_np16'
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/p/gpfs1/mtcampbe/CEESD/AutomatedTesting/RepoMonitoring/pilot-timing/emirge/miniforge3/envs/timing.pilot/lib/python3.12/site-packages/mpi4py/__main__.py", line 7, in <module>
    main()
  File "/p/gpfs1/mtcampbe/CEESD/AutomatedTesting/RepoMonitoring/pilot-timing/emirge/miniforge3/envs/timing.pilot/lib/python3.12/site-packages/mpi4py/run.py", line 214, in main
    run_command_line(args)
  File "/p/gpfs1/mtcampbe/CEESD/AutomatedTesting/RepoMonitoring/pilot-timing/emirge/miniforge3/envs/timing.pilot/lib/python3.12/site-packages/mpi4py/run.py", line 46, in run_command_line
    run_path(sys.argv[0], run_name='__main__')
  File "<frozen runpy>", line 287, in run_path
  File "<frozen runpy>", line 98, in _run_module_code
  File "<frozen runpy>", line 88, in _run_code
  File "driver.py", line 51, in <module>
    from mirgecom.simutil import ApplicationOptionsError
  File "/p/gpfs1/mtcampbe/CEESD/AutomatedTesting/RepoMonitoring/pilot-timing/emirge/mirgecom/mirgecom/simutil.py", line 87, in <module>
    import grudge.op as op
  File "/p/gpfs1/mtcampbe/CEESD/AutomatedTesting/RepoMonitoring/pilot-timing/emirge/grudge/grudge/op.py", line 132, in <module>
    from grudge.trace_pair import (
  File "/p/gpfs1/mtcampbe/CEESD/AutomatedTesting/RepoMonitoring/pilot-timing/emirge/grudge/grudge/trace_pair.py", line 107, in <module>
    from pytools.persistent_dict import Hash, KeyBuilder
ImportError: cannot import name 'Hash' from 'pytools.persistent_dict' (/p/gpfs1/mtcampbe/CEESD/AutomatedTesting/RepoMonitoring/pilot-timing/emirge/pytools/pytools/persistent_dict.py)
++ return_code=1
++ set +x
mv: cannot stat ‘viz_data’: No such file or directory
mv: cannot stat ‘restart_data’**  scalability_test/prediction-scalability_np16 failed.
Thu May  8 12:35:04 PDT 2025
: No such file or directory
Scaling/timing tests done.
Passing tests: 
Failing tests:  scalability_test/prediction-scalability_np16

------------------------------------------------------------
Sender: LSF System <lsfadmin@lassen710>
Subject: Job 7240157: <scale16> in cluster <lassen> Exited

Job <scale16> was submitted from host <lassen709> by user <mtcampbe> in cluster <lassen> at Thu May  8 12:33:51 2025
Job was executed on host(s) <1*lassen710>, in queue <pdebug>, as user <mtcampbe> in cluster <lassen> at Thu May  8 12:33:54 2025
                            <40*lassen30>
                            <40*lassen19>
                            <40*lassen18>
                            <40*lassen17>
</g/g17/mtcampbe> was used as the home directory.
</p/gpfs1/mtcampbe/CEESD/AutomatedTesting/RepoMonitoring/pilot-timing/y3-prediction-scaling-run/scalability_test> was used as the working directory.
Started at Thu May  8 12:33:54 2025
Terminated at Thu May  8 12:36:05 2025
Results reported at Thu May  8 12:36:05 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
#BSUB -nnodes 4
#BSUB -G uiuc
#BSUB -W 120
#BSUB -J scale16
#BSUB -q pdebug
#BSUB -o scal16.txt

source ../emirge/config/activate_env.sh
source ../emirge/mirgecom/scripts/mirge-testing-env.sh
source ../scripts/multi_scalability.sh -p ../ -s 16 -n 16

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   0.20 sec.
    Max Memory :                                 126 MB
    Average Memory :                             74.93 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   1634 MB
    Max Processes :                              4
    Max Threads :                                27
    Run time :                                   131 sec.
    Turnaround time :                            134 sec.

The output (if any) is above this job summary.

