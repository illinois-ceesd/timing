Customizing shell for Lassen.

The following have been reloaded with a version change:
  1) base-gcc/8.3.1 => base-gcc/11.2.1


Lmod is automatically replacing "clang/ibm-16.0.6-cuda-11.2.0-gcc-8.3.1" with
"gcc/10.2.1".


Due to MODULEPATH changes, the following have been reloaded:
  1) spectrum-mpi/rolling-release

Activating 'timing.main' environment for '/p/gpfs1/mtcampbe/CEESD/AutomatedTesting/MIRGE-Timing/timing/emirge/miniforge3/bin/conda'.
/p/gpfs1/mtcampbe/CEESD/AutomatedTesting/MIRGE-Timing/timing/y3-prediction-scaling-run/scalability_test
/p/gpfs1/mtcampbe/CEESD/AutomatedTesting/MIRGE-Timing/timing/y3-prediction-scaling-run/scalability_test
Driver directory: /p/gpfs1/mtcampbe/CEESD/AutomatedTesting/MIRGE-Timing/timing/y3-prediction-scaling-run
Thu Jun  5 23:33:59 PDT 2025
Running parallel timing tests...
* Running prediction-scalability test in scalability_test.
** Running prediction-scalability_np1 on 1 ranks.
++ runoptions='-n 1'
++ [[ ! -z '' ]]
++ jsrun -g 1 -a 1 -n 1 bash /p/gpfs1/mtcampbe/CEESD/AutomatedTesting/MIRGE-Timing/timing/y3-prediction-scaling-run/emirge/mirgecom/scripts/lassen-parallel-spawner.sh python -u -O -m mpi4py driver.py -c prediction-scalability_np1 -g /p/gpfs1/mtcampbe/CEESD/AutomatedTesting/MIRGE-Timing/timing/y3-prediction-scaling-run/scalability_test/log_data -i run_params_np1.yaml --log --lazy
Custom casename 'prediction-scalability_np1'
Using user input from file: run_params_np1.yaml
Running driver.py

2025-06-05 23:34:07,410 - INFO - grudge.array_context - get_reasonable_array_context_class: MPIFusionContractorArrayContext lazy=True distributed=True device-parallel=True
2025-06-05 23:34:07,911 - WARNING - py.warnings - /p/gpfs1/mtcampbe/CEESD/AutomatedTesting/MIRGE-Timing/timing/y3-prediction-scaling-run/y3prediction/wall.py:37: UserWarning: <class 'y3prediction.wall.WallVars'> does not have __array_ufunc__ set. This will cause numpy to attempt broadcasting, in a way that is likely undesired. To avoid this, set __array_ufunc__ = None in <class 'y3prediction.wall.WallVars'>.
  @with_container_arithmetic(bcast_obj_array=False,

_mm.init failed
[lassen281:89664] Error: common_pami.c:1094 - ompi_common_pami_init() 0: Unable to create 1 PAMI communication context(s) rc=1
--------------------------------------------------------------------------
No components were able to be opened in the pml framework.

This typically means that either no components of this type were
installed, or none of the installed components can be loaded.
Sometimes this means that shared libraries required by these
components are unable to be found/loaded.

  Host:      lassen281
  Framework: pml
--------------------------------------------------------------------------
[lassen281:89664] PML pami cannot be selected
++ return_code=1
++ set +x
**  scalability_test/prediction-scalability_np1 failed.
** Running prediction-scalability_np2 on 2 ranks.
++ runoptions='-n 2'
++ [[ ! -z '' ]]
++ jsrun -g 1 -a 1 -n 2 bash /p/gpfs1/mtcampbe/CEESD/AutomatedTesting/MIRGE-Timing/timing/y3-prediction-scaling-run/emirge/mirgecom/scripts/lassen-parallel-spawner.sh python -u -O -m mpi4py driver.py -c prediction-scalability_np2 -g /p/gpfs1/mtcampbe/CEESD/AutomatedTesting/MIRGE-Timing/timing/y3-prediction-scaling-run/scalability_test/log_data -i run_params_np2.yaml --log --lazy
Custom casename 'prediction-scalability_np2'
Custom casename 'prediction-scalability_np2'
Using user input from file: run_params_np2.yaml
Running driver.py

2025-06-05 23:34:10,052 - INFO - grudge.array_context - get_reasonable_array_context_class: MPIFusionContractorArrayContext lazy=True distributed=True device-parallel=True
2025-06-05 23:34:10,088 - INFO - grudge.array_context - get_reasonable_array_context_class: MPIFusionContractorArrayContext lazy=True distributed=True device-parallel=True
Using user input from file: run_params_np2.yaml
Running driver.py

2025-06-05 23:34:10,113 - WARNING - py.warnings - /p/gpfs1/mtcampbe/CEESD/AutomatedTesting/MIRGE-Timing/timing/y3-prediction-scaling-run/y3prediction/wall.py:37: UserWarning: <class 'y3prediction.wall.WallVars'> does not have __array_ufunc__ set. This will cause numpy to attempt broadcasting, in a way that is likely undesired. To avoid this, set __array_ufunc__ = None in <class 'y3prediction.wall.WallVars'>.
  @with_container_arithmetic(bcast_obj_array=False,

2025-06-05 23:34:10,147 - WARNING - py.warnings - /p/gpfs1/mtcampbe/CEESD/AutomatedTesting/MIRGE-Timing/timing/y3-prediction-scaling-run/y3prediction/wall.py:37: UserWarning: <class 'y3prediction.wall.WallVars'> does not have __array_ufunc__ set. This will cause numpy to attempt broadcasting, in a way that is likely undesired. To avoid this, set __array_ufunc__ = None in <class 'y3prediction.wall.WallVars'>.
  @with_container_arithmetic(bcast_obj_array=False,

_mm.init failed
[lassen281:89770] Error: common_pami.c:1094 - ompi_common_pami_init() 0: Unable to create 1 PAMI communication context(s) rc=1
--------------------------------------------------------------------------
No components were able to be opened in the pml framework.

This typically means that either no components of this type were
installed, or none of the installed components can be loaded.
Sometimes this means that shared libraries required by these
components are unable to be found/loaded.

  Host:      lassen281
  Framework: pml
--------------------------------------------------------------------------
[lassen281:89770] PML pami cannot be selected
++ return_code=1
++ set +x
mv: cannot stat ‘viz_data’: No such file or directory
mv: cannot stat ‘restart_data’: No such file or directory
**  scalability_test/prediction-scalability_np2 failed.
** Running prediction-scalability_np4 on 4 ranks.
++ runoptions='-n 4'
++ [[ ! -z '' ]]
++ jsrun -g 1 -a 1 -n 4 bash /p/gpfs1/mtcampbe/CEESD/AutomatedTesting/MIRGE-Timing/timing/y3-prediction-scaling-run/emirge/mirgecom/scripts/lassen-parallel-spawner.sh python -u -O -m mpi4py driver.py -c prediction-scalability_np4 -g /p/gpfs1/mtcampbe/CEESD/AutomatedTesting/MIRGE-Timing/timing/y3-prediction-scaling-run/scalability_test/log_data -i run_params_np4.yaml --log --lazy
Custom casename 'prediction-scalability_np4'
Custom casename 'prediction-scalability_np4'
Custom casename 'prediction-scalability_np4'
Custom casename 'prediction-scalability_np4'
Using user input from file: run_params_np4.yaml
Running driver.py

2025-06-05 23:34:11,757 - INFO - grudge.array_context - get_reasonable_array_context_class: MPIFusionContractorArrayContext lazy=True distributed=True device-parallel=True
Using user input from file: run_params_np4.yaml
Running driver.py

2025-06-05 23:34:11,757 - INFO - grudge.array_context - get_reasonable_array_context_class: MPIFusionContractorArrayContext lazy=True distributed=True device-parallel=True
2025-06-05 23:34:11,816 - WARNING - py.warnings - /p/gpfs1/mtcampbe/CEESD/AutomatedTesting/MIRGE-Timing/timing/y3-prediction-scaling-run/y3prediction/wall.py:37: UserWarning: <class 'y3prediction.wall.WallVars'> does not have __array_ufunc__ set. This will cause numpy to attempt broadcasting, in a way that is likely undesired. To avoid this, set __array_ufunc__ = None in <class 'y3prediction.wall.WallVars'>.
  @with_container_arithmetic(bcast_obj_array=False,

2025-06-05 23:34:11,818 - WARNING - py.warnings - /p/gpfs1/mtcampbe/CEESD/AutomatedTesting/MIRGE-Timing/timing/y3-prediction-scaling-run/y3prediction/wall.py:37: UserWarning: <class 'y3prediction.wall.WallVars'> does not have __array_ufunc__ set. This will cause numpy to attempt broadcasting, in a way that is likely undesired. To avoid this, set __array_ufunc__ = None in <class 'y3prediction.wall.WallVars'>.
  @with_container_arithmetic(bcast_obj_array=False,

2025-06-05 23:34:11,847 - INFO - grudge.array_context - get_reasonable_array_context_class: MPIFusionContractorArrayContext lazy=True distributed=True device-parallel=True
2025-06-05 23:34:11,847 - INFO - grudge.array_context - get_reasonable_array_context_class: MPIFusionContractorArrayContext lazy=True distributed=True device-parallel=True
Using user input from file: run_params_np4.yaml
Running driver.py

Using user input from file: run_params_np4.yaml
Running driver.py

2025-06-05 23:34:11,909 - WARNING - py.warnings - /p/gpfs1/mtcampbe/CEESD/AutomatedTesting/MIRGE-Timing/timing/y3-prediction-scaling-run/y3prediction/wall.py:37: UserWarning: <class 'y3prediction.wall.WallVars'> does not have __array_ufunc__ set. This will cause numpy to attempt broadcasting, in a way that is likely undesired. To avoid this, set __array_ufunc__ = None in <class 'y3prediction.wall.WallVars'>.
  @with_container_arithmetic(bcast_obj_array=False,

2025-06-05 23:34:11,909 - WARNING - py.warnings - /p/gpfs1/mtcampbe/CEESD/AutomatedTesting/MIRGE-Timing/timing/y3-prediction-scaling-run/y3prediction/wall.py:37: UserWarning: <class 'y3prediction.wall.WallVars'> does not have __array_ufunc__ set. This will cause numpy to attempt broadcasting, in a way that is likely undesired. To avoid this, set __array_ufunc__ = None in <class 'y3prediction.wall.WallVars'>.
  @with_container_arithmetic(bcast_obj_array=False,

_mm.init failed
[lassen281:89828] Error: common_pami.c:1094 - ompi_common_pami_init() 1: Unable to create 1 PAMI communication context(s) rc=1
_mm.init failed
[lassen281:89827] Error: common_pami.c:1094 - ompi_common_pami_init() 0: Unable to create 1 PAMI communication context(s) rc=1
--------------------------------------------------------------------------
No components were able to be opened in the pml framework.

This typically means that either no components of this type were
installed, or none of the installed components can be loaded.
Sometimes this means that shared libraries required by these
components are unable to be found/loaded.

  Host:      lassen281
  Framework: pml
--------------------------------------------------------------------------
[lassen281:89827] PML pami cannot be selected
--------------------------------------------------------------------------
No components were able to be opened in the pml framework.

This typically means that either no components of this type were
installed, or none of the installed components can be loaded.
Sometimes this means that shared libraries required by these
components are unable to be found/loaded.

  Host:      lassen281
  Framework: pml
--------------------------------------------------------------------------
[lassen281:89828] PML pami cannot be selected
++ return_code=1
++ set +x
mv: cannot stat ‘viz_data’: No such file or directory
mv: cannot stat ‘restart_data’: No such file or directory
**  scalability_test/prediction-scalability_np4 failed.
Thu Jun  5 23:34:11 PDT 2025
Scaling/timing tests done.
Passing tests: 
Failing tests:  scalability_test/prediction-scalability_np1 scalability_test/prediction-scalability_np2 scalability_test/prediction-scalability_np4

------------------------------------------------------------
Sender: LSF System <lsfadmin@lassen710>
Subject: Job 7316956: <scale4> in cluster <lassen> Exited

Job <scale4> was submitted from host <lassen709> by user <mtcampbe> in cluster <lassen> at Thu Jun  5 23:24:00 2025
Job was executed on host(s) <1*lassen710>, in queue <pbatch>, as user <mtcampbe> in cluster <lassen> at Thu Jun  5 23:33:51 2025
                            <40*lassen281>
</g/g17/mtcampbe> was used as the home directory.
</p/gpfs1/mtcampbe/CEESD/AutomatedTesting/MIRGE-Timing/timing/y3-prediction-scaling-run/scalability_test> was used as the working directory.
Started at Thu Jun  5 23:33:51 2025
Terminated at Thu Jun  5 23:34:12 2025
Results reported at Thu Jun  5 23:34:12 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
#BSUB -nnodes 1
#BSUB -G uiuc
#BSUB -W 180
#BSUB -J scale4
#BSUB -q pbatch
#BSUB -o scal4.txt

source ../emirge/config/activate_env.sh
source ../emirge/mirgecom/scripts/mirge-testing-env.sh
source ../scripts/multi_scalability.sh -p ../ -n 4



------------------------------------------------------------

Exited with exit code 3.

Resource usage summary:

    CPU time :                                   0.23 sec.
    Max Memory :                                 192 MB
    Average Memory :                             192.00 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   354 MB
    Max Processes :                              1
    Max Threads :                                1
    Run time :                                   20 sec.
    Turnaround time :                            612 sec.

The output (if any) is above this job summary.

