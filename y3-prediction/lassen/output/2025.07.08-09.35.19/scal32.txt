Customizing shell for Lassen.

The following have been reloaded with a version change:
  1) base-gcc/8.3.1 => base-gcc/11.2.1


Lmod is automatically replacing "clang/ibm-16.0.6-cuda-11.2.0-gcc-8.3.1" with
"gcc/10.2.1".


Due to MODULEPATH changes, the following have been reloaded:
  1) spectrum-mpi/rolling-release

/g/g17/mtcampbe/.lsbatch/1751981834.7425272.shell: line 9: ../emirge/config/activate_env.sh: No such file or directory
/g/g17/mtcampbe/.lsbatch/1751981834.7425272.shell: line 10: ../emirge/mirgecom/scripts/mirge-testing-env.sh: No such file or directory
/p/gpfs1/mtcampbe/CEESD/AutomatedTesting/RepoMonitoring/pilot-timing/y3-prediction-scaling-run/scalability_test
Driver directory: /p/gpfs1/mtcampbe/CEESD/AutomatedTesting/RepoMonitoring/pilot-timing/y3-prediction-scaling-run
Tue Jul  8 09:04:38 PDT 2025
Running parallel timing tests...
* Running prediction-scalability test in scalability_test.
** Running prediction-scalability_np32 on 32 ranks.
++ runoptions='-n 32'
++ [[ ! -z '' ]]
++ mpiexec -n 32 python -u -O -m mpi4py driver.py -c prediction-scalability_np32 -g /p/gpfs1/mtcampbe/CEESD/AutomatedTesting/RepoMonitoring/pilot-timing/y3-prediction-scaling-run/scalability_test/log_data -i run_params_np32.yaml --log --lazy
/p/gpfs1/mtcampbe/CEESD/AutomatedTesting/RepoMonitoring/pilot-timing/emirge/miniforge3/bin/python: No module named mpi4py
/p/gpfs1/mtcampbe/CEESD/AutomatedTesting/RepoMonitoring/pilot-timing/emirge/miniforge3/bin/python: No module named mpi4py
/p/gpfs1/mtcampbe/CEESD/AutomatedTesting/RepoMonitoring/pilot-timing/emirge/miniforge3/bin/python: No module named mpi4py
/p/gpfs1/mtcampbe/CEESD/AutomatedTesting/RepoMonitoring/pilot-timing/emirge/miniforge3/bin/python: No module named mpi4py
/p/gpfs1/mtcampbe/CEESD/AutomatedTesting/RepoMonitoring/pilot-timing/emirge/miniforge3/bin/python: No module named mpi4py
/p/gpfs1/mtcampbe/CEESD/AutomatedTesting/RepoMonitoring/pilot-timing/emirge/miniforge3/bin/python: No module named mpi4py
/p/gpfs1/mtcampbe/CEESD/AutomatedTesting/RepoMonitoring/pilot-timing/emirge/miniforge3/bin/python: No module named mpi4py
/p/gpfs1/mtcampbe/CEESD/AutomatedTesting/RepoMonitoring/pilot-timing/emirge/miniforge3/bin/python: No module named mpi4py
/p/gpfs1/mtcampbe/CEESD/AutomatedTesting/RepoMonitoring/pilot-timing/emirge/miniforge3/bin/python: No module named mpi4py
/p/gpfs1/mtcampbe/CEESD/AutomatedTesting/RepoMonitoring/pilot-timing/emirge/miniforge3/bin/python: No module named mpi4py
/p/gpfs1/mtcampbe/CEESD/AutomatedTesting/RepoMonitoring/pilot-timing/emirge/miniforge3/bin/python: No module named mpi4py
/p/gpfs1/mtcampbe/CEESD/AutomatedTesting/RepoMonitoring/pilot-timing/emirge/miniforge3/bin/python: No module named mpi4py
/p/gpfs1/mtcampbe/CEESD/AutomatedTesting/RepoMonitoring/pilot-timing/emirge/miniforge3/bin/python: No module named mpi4py
/p/gpfs1/mtcampbe/CEESD/AutomatedTesting/RepoMonitoring/pilot-timing/emirge/miniforge3/bin/python: No module named mpi4py
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
/p/gpfs1/mtcampbe/CEESD/AutomatedTesting/RepoMonitoring/pilot-timing/emirge/miniforge3/bin/python: No module named mpi4py
/p/gpfs1/mtcampbe/CEESD/AutomatedTesting/RepoMonitoring/pilot-timing/emirge/miniforge3/bin/python: No module named mpi4py
/p/gpfs1/mtcampbe/CEESD/AutomatedTesting/RepoMonitoring/pilot-timing/emirge/miniforge3/bin/python: No module named mpi4py
/p/gpfs1/mtcampbe/CEESD/AutomatedTesting/RepoMonitoring/pilot-timing/emirge/miniforge3/bin/python: No module named mpi4py
/p/gpfs1/mtcampbe/CEESD/AutomatedTesting/RepoMonitoring/pilot-timing/emirge/miniforge3/bin/python: No module named mpi4py
/p/gpfs1/mtcampbe/CEESD/AutomatedTesting/RepoMonitoring/pilot-timing/emirge/miniforge3/bin/python: No module named mpi4py
/p/gpfs1/mtcampbe/CEESD/AutomatedTesting/RepoMonitoring/pilot-timing/emirge/miniforge3/bin/python: No module named mpi4py
/p/gpfs1/mtcampbe/CEESD/AutomatedTesting/RepoMonitoring/pilot-timing/emirge/miniforge3/bin/python: No module named mpi4py
/p/gpfs1/mtcampbe/CEESD/AutomatedTesting/RepoMonitoring/pilot-timing/emirge/miniforge3/bin/python: No module named mpi4py
/p/gpfs1/mtcampbe/CEESD/AutomatedTesting/RepoMonitoring/pilot-timing/emirge/miniforge3/bin/python: No module named mpi4py
/p/gpfs1/mtcampbe/CEESD/AutomatedTesting/RepoMonitoring/pilot-timing/emirge/miniforge3/bin/python: No module named mpi4py
/p/gpfs1/mtcampbe/CEESD/AutomatedTesting/RepoMonitoring/pilot-timing/emirge/miniforge3/bin/python: No module named mpi4py
/p/gpfs1/mtcampbe/CEESD/AutomatedTesting/RepoMonitoring/pilot-timing/emirge/miniforge3/bin/python: No module named mpi4py
/p/gpfs1/mtcampbe/CEESD/AutomatedTesting/RepoMonitoring/pilot-timing/emirge/miniforge3/bin/python: No module named mpi4py
/p/gpfs1/mtcampbe/CEESD/AutomatedTesting/RepoMonitoring/pilot-timing/emirge/miniforge3/bin/python: No module named mpi4py
/p/gpfs1/mtcampbe/CEESD/AutomatedTesting/RepoMonitoring/pilot-timing/emirge/miniforge3/bin/python: No module named mpi4py
/p/gpfs1/mtcampbe/CEESD/AutomatedTesting/RepoMonitoring/pilot-timing/emirge/miniforge3/bin/python: No module named mpi4py
/p/gpfs1/mtcampbe/CEESD/AutomatedTesting/RepoMonitoring/pilot-timing/emirge/miniforge3/bin/python: No module named mpi4py
------------------------------------------------------------
A process or daemon was unable to complete a TCP connection
to another process:
  Local host:    lassen595
  Remote host:   lassen710
This is usually caused by a firewall on the remote host. Please
check that any firewall (e.g., iptables) has been disabled and
try again.
------------------------------------------------------------
------------------------------------------------------------
A process or daemon was unable to complete a TCP connection
to another process:
  Local host:    lassen595
  Remote host:   lassen750
This is usually caused by a firewall on the remote host. Please
check that any firewall (e.g., iptables) has been disabled and
try again.
------------------------------------------------------------
------------------------------------------------------------
A process or daemon was unable to complete a TCP connection
to another process:
  Local host:    lassen595
  Remote host:   lassen293
This is usually caused by a firewall on the remote host. Please
check that any firewall (e.g., iptables) has been disabled and
try again.
------------------------------------------------------------
------------------------------------------------------------
A process or daemon was unable to complete a TCP connection
to another process:
  Local host:    lassen595
  Remote host:   lassen782
This is usually caused by a firewall on the remote host. Please
check that any firewall (e.g., iptables) has been disabled and
try again.
------------------------------------------------------------
------------------------------------------------------------
A process or daemon was unable to complete a TCP connection
to another process:
  Local host:    lassen595
  Remote host:   lassen303
This is usually caused by a firewall on the remote host. Please
check that any firewall (e.g., iptables) has been disabled and
try again.
------------------------------------------------------------
------------------------------------------------------------
A process or daemon was unable to complete a TCP connection
to another process:
  Local host:    lassen595
  Remote host:   lassen155
This is usually caused by a firewall on the remote host. Please
check that any firewall (e.g., iptables) has been disabled and
try again.
------------------------------------------------------------
------------------------------------------------------------
A process or daemon was unable to complete a TCP connection
to another process:
  Local host:    lassen595
  Remote host:   lassen310
This is usually caused by a firewall on the remote host. Please
check that any firewall (e.g., iptables) has been disabled and
try again.
------------------------------------------------------------
------------------------------------------------------------
A process or daemon was unable to complete a TCP connection
to another process:
  Local host:    lassen595
  Remote host:   lassen804
This is usually caused by a firewall on the remote host. Please
check that any firewall (e.g., iptables) has been disabled and
try again.
------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[4257,1],15]
  Exit code:    1
--------------------------------------------------------------------------
++ return_code=1
++ set +x
mv: cannot stat ‘viz_data’: No such file or directory
mv: cannot stat ‘restart_data’: No such file or directory
**  scalability_test/prediction-scalability_np32 failed.
Tue Jul  8 09:04:44 PDT 2025
Scaling/timing tests done.
Passing tests: 
Failing tests:  scalability_test/prediction-scalability_np32

------------------------------------------------------------
Sender: LSF System <lsfadmin@lassen710>
Subject: Job 7425272: <scale32> in cluster <lassen> Exited

Job <scale32> was submitted from host <lassen709> by user <mtcampbe> in cluster <lassen> at Tue Jul  8 06:37:14 2025
Job was executed on host(s) <1*lassen710>, in queue <pbatch>, as user <mtcampbe> in cluster <lassen> at Tue Jul  8 09:04:29 2025
                            <40*lassen595>
                            <40*lassen750>
                            <40*lassen293>
                            <40*lassen782>
                            <40*lassen303>
                            <40*lassen155>
                            <40*lassen310>
                            <40*lassen804>
</g/g17/mtcampbe> was used as the home directory.
</p/gpfs1/mtcampbe/CEESD/AutomatedTesting/RepoMonitoring/pilot-timing/y3-prediction-scaling-run/scalability_test> was used as the working directory.
Started at Tue Jul  8 09:04:29 2025
Terminated at Tue Jul  8 09:04:45 2025
Results reported at Tue Jul  8 09:04:45 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
#BSUB -nnodes 8
#BSUB -G uiuc
#BSUB -W 120
#BSUB -J scale32
#BSUB -q pbatch
#BSUB -o scal32.txt

source ../emirge/config/activate_env.sh
source ../emirge/mirgecom/scripts/mirge-testing-env.sh
source ../scripts/multi_scalability.sh -p ../ -s 32 -n 32

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   0.14 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              -
    Max Threads :                                -
    Run time :                                   19 sec.
    Turnaround time :                            8851 sec.

The output (if any) is above this job summary.

