<<Error: tweaked_jsm_wait_for_ready returned 22 to lexec>>

This typically indicates one of the following five issues:
 1) A long running command in .bashrc such as SPACK or CONDA init
 2) Disabling ssh key checks by creating ~/.ssh/setup_ssh_keys_skip_tests
 3) Incompatible ssh keys or ~/.ssh/config (Did you recently change either?)
 4) Setting TMP or TMP_DIR to a dir that doesn't exist on compute nodes
 5) A system problem where NFS mounts or SSH daemons are broken on a node

NOTE: Only 5) is a system problem. The rest require changing user files to fix

5) Starting JSM diagnostics for allocation 5867894 from lassen710
Testing nodes lassen[80,251,257,414,581,583,743-744]
Launched  Fri Oct 27 04:21:18 PDT 2023
Completed Fri Oct 27 04:21:19 PDT 2023

NOTE: Any output between 'Launched' and 'Completed' confirms a system problem
      that should be reported to John Gyllenhaal gyllen@llnl.gov and please
      cc lc-hotline@llnl.gov and include the above output!

NOTE: No output suggests it is an user solvable issue, such as listed in 1-4

Starting user file diagnostics printing hopefully useful info on 1-4:
1) Running: grep -i -e spack/setup -e conda ~/.bashrc
#    __conda_setup="$('/usr/workspace/wsa/xpacc/Users/mtcampbe/CEESD/Quartz/Miniconda3/bin/conda' 'shell.bas#h' 'hook' 2> /dev/null)"
#        eval "$__conda_setup"
#        if [ -f "/usr/workspace/wsa/xpacc/Users/mtcampbe/CEESD/Quartz/Miniconda3/etc/profile.d/conda.sh" ]; then
#            . "/usr/workspace/wsa/xpacc/Users/mtcampbe/CEESD/Quartz/Miniconda3/etc/profile.d/conda.sh"
#            export PATH="/usr/workspace/wsa/xpacc/Users/mtcampbe/CEESD/Quartz/Miniconda3/bin:$PATH"
#    unset __conda_setup
# >>> conda initialize >>>
# !! Contents within this block are managed by 'conda init' !!
# <<< conda initialize <<<
We have found that python scripts that typically take 3 seconds to run can
intermittently run much longer, triggering the 60 second JSM launch timeout.
Restricting to interactive with 'if [[ huB == *i* ]]; then <cmd> fi' may help

2) Running: ls -l  ~/.ssh/setup_ssh_keys_skip_tests
/usr/bin/ls: cannot access /g/g17/mtcampbe/.ssh/setup_ssh_keys_skip_tests: No such file or directory
Excellent, we do not want ~/.ssh/setup_ssh_keys_skip_tests to exist

3a) Running: ls -lt ~/.ssh | head -6
total 44
-rw-r--r-- 1 mtcampbe mtcampbe 12583 Mar 25  2023 known_hosts
-rw-r--r-- 1 mtcampbe mtcampbe 13023 Mar 24  2023 known_hosts~
-rw------- 1 mtcampbe mtcampbe     0 May 14  2022 setup_ssh_keys_verified
-rw------- 1 mtcampbe mtcampbe   397 Mar 10  2016 authorized_keys
-rw------- 1 mtcampbe mtcampbe   397 Mar 10  2016 id_rsa.pub

3b) Running: setup_ssh_keys -v
Testing /g/g17/mtcampbe permissions (currently drwx------)
Testing /g/g17/mtcampbe/.ssh permissions (currently 700)
Testing /g/g17/mtcampbe/.ssh/authorized_keys permissions (currently -rw-------)
Testing /g/g17/mtcampbe/.ssh/id_rsa permissions (currently -rw-------)
Ssh key test on lassen passed
Touching file /g/g17/mtcampbe/.ssh/setup_ssh_keys_verified to cache

4a) Running: grep TMP ~/.bash* ~/.profile* ~/.csh* ~/.login*

4b) Running: ls -lt  ~/.bash* ~/.profile* ~/.csh* ~/.login* | head -3
-rw------- 1 mtcampbe mtcampbe   296 Sep 14 07:59 /g/g17/mtcampbe/.bash_profile
-rw------- 1 mtcampbe mtcampbe   787 Aug 31 12:20 /g/g17/mtcampbe/.bashrc
-rw------- 1 mtcampbe mtcampbe   296 Nov  9  2022 /g/g17/mtcampbe/.bash_profile~

NOTE: Until the problem is found and fixed, getting a new allocation
      with different nodes can be an effective temporary workaround.

<<If it is a confirmed system problem or if you need help solving the issue>>
<<please contact John Gyllenhaal gyllen@llnl.gov and cc lc-hotline@llnl.gov>>
<<Please include all the output above in the email to help us find the problem>>

<<Ending JSM diagnostics, please include all of the above if emailing for help>>
<<Exiting job due to node issues detected>>

------------------------------------------------------------
Sender: LSF System <lsfadmin@lassen710>
Subject: Job 5363319: <scale32> in cluster <lassen> Exited

Job <scale32> was submitted from host <lassen708> by user <mtcampbe> in cluster <lassen> at Fri Oct 27 00:14:53 2023
Job was executed on host(s) <1*lassen710>, in queue <pbatch>, as user <mtcampbe> in cluster <lassen> at Fri Oct 27 04:20:12 2023
                            <40*lassen251>
                            <40*lassen581>
                            <40*lassen583>
                            <40*lassen257>
                            <40*lassen80>
                            <40*lassen743>
                            <40*lassen414>
                            <40*lassen744>
</g/g17/mtcampbe> was used as the home directory.
</p/gpfs1/mtcampbe/CEESD/AutomatedTesting/MIRGE-Timing/timing/y3-prediction-scaling-run/scalability_test> was used as the working directory.
Started at Fri Oct 27 04:20:12 2023
Terminated at Fri Oct 27 04:21:20 2023
Results reported at Fri Oct 27 04:21:20 2023

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
#BSUB -nnodes 8
#BSUB -G uiuc
#BSUB -W 120
#BSUB -J scale32
#BSUB -q pbatch
#BSUB -o scal32.txt

source ../emirge/config/activate_env.sh
source ../emirge/mirgecom/scripts/mirge-testing-env.sh
source ../scripts/multi_scalability.sh -p ../ -s 32 -n 32

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   101.77 sec.
    Max Memory :                                 93 MB
    Average Memory :                             93.00 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   1765 MB
    Max Processes :                              7
    Max Threads :                                31
    Run time :                                   68 sec.
    Turnaround time :                            14787 sec.

The output (if any) is above this job summary.

